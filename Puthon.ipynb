{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Puthon.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SupreetRonad/Analysis-of-Employee-Attrition-Rate-and-Performance/blob/main/Puthon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83qVT1gI8O-i",
        "outputId": "6ba893fa-acb8-4ae3-8454-cb58ad925741"
      },
      "source": [
        "!pip install PyPDF2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting PyPDF2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/01/68fcc0d43daf4c6bdbc6b33cc3f77bda531c86b174cac56ef0ffdb96faab/PyPDF2-1.26.0.tar.gz (77kB)\n",
            "\r\u001b[K     |████▎                           | 10kB 14.4MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 20kB 20.2MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 30kB 24.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 40kB 23.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 51kB 16.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 61kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 71kB 12.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 6.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: PyPDF2\n",
            "  Building wheel for PyPDF2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyPDF2: filename=PyPDF2-1.26.0-cp37-none-any.whl size=61102 sha256=1ab69c5db5790ad1e237ba07bcfce7998361ebc9202e56ad58f3969ba0a648b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/53/84/19/35bc977c8bf5f0c23a8a011aa958acd4da4bbd7a229315c1b7\n",
            "Successfully built PyPDF2\n",
            "Installing collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-1.26.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQJ6K1jjCXE6",
        "outputId": "b37e8399-2bd7-4af6-c33a-3798a763d78d"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnioYEEECnnU",
        "outputId": "df63d0f6-df8e-45e4-fb22-1118ee0584a8"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbTImyeBBqIo"
      },
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "def get_wordnet_pos(pos_tag):\n",
        "    if pos_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif pos_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif pos_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif pos_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "    \n",
        "import string\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def clean_text(text):\n",
        "    # lower text\n",
        "    text = text.lower()\n",
        "    # tokenize text and remove puncutation\n",
        "    text = [word.strip(string.punctuation) for word in text.split(\" \")]\n",
        "    # remove words that contain numbers\n",
        "    text = [word for word in text if not any(c.isdigit() for c in word)]\n",
        "    # remove stop words\n",
        "    stop = stopwords.words('english')\n",
        "    text = [x for x in text if x not in stop]\n",
        "    # remove empty tokens\n",
        "    text = [t for t in text if len(t) > 0]\n",
        "    # pos tag text\n",
        "    pos_tags = pos_tag(text)\n",
        "    # lemmatize text\n",
        "    text = [WordNetLemmatizer().lemmatize(t[0], get_wordnet_pos(t[1])) for t in pos_tags]\n",
        "    # remove words with only one letter\n",
        "    text = [t for t in text if len(t) > 1]\n",
        "    # join all\n",
        "    text = \" \".join(text)\n",
        "    return(text)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvWx9MzUH7-n",
        "outputId": "08756cfd-a16b-41cb-f6c8-6f08a28a3da0"
      },
      "source": [
        "f = open(\"/content/message.txt\", \"r\")\n",
        "keywords=clean_text(f.read()).split()\n",
        "dict={}\n",
        "for i in keywords:\n",
        "  dict[i]=0\n",
        "count = 0\n",
        "print(dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'accident': 0, 'resuscitation': 0, 'renal': 0, 'failure': 0, 'ulcer': 0, 'surgery': 0, 'operation': 0, 'acquire': 0, 'sore': 0, 'reaction': 0, 'pneumonia': 0, 'tube': 0, 'difficile': 0, 'fluid': 0, 'infection': 0, 'distress': 0, 'sepsis': 0, 'breath': 0, 'vein': 0, 'thrombosis': 0, 'sugar': 0, 'consult': 0, 'status': 0, 'eruption': 0, 'rash': 0, 'room': 0, 'rbc': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lR7hD1LDPDFj"
      },
      "source": [
        "def search(list, platform):\n",
        "    for i in range(len(list)):\n",
        "        if list[i] == platform:\n",
        "            return True\n",
        "    return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iyex18wS68Cp",
        "outputId": "e57a2c9d-4967-4bfb-9d7e-42807d5635c8"
      },
      "source": [
        "# importing required modules\n",
        "import PyPDF2\n",
        "\n",
        "# creating a pdf file object\n",
        "pdfFileObj = open('/content/mb-das-may-2011.pdf', 'rb')\n",
        "\n",
        "# creating a pdf reader object\n",
        "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
        "\n",
        "# printing number of pages in pdf file\n",
        "\n",
        "for i in range(pdfReader.numPages):\n",
        "  # creating a page object\n",
        "  pageObj = pdfReader.getPage(i)\n",
        "  my=clean_text(pageObj.extractText()).split()\n",
        "  # extracting text from page\n",
        "  for i in dict:\n",
        "    if i in my:\n",
        "      dict[i]=1\n",
        "\n",
        "  # closing the pdf file object\n",
        "pdfFileObj.close()\n",
        "print(dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'accident': 1, 'resuscitation': 1, 'renal': 1, 'failure': 1, 'ulcer': 1, 'surgery': 1, 'operation': 1, 'acquire': 1, 'sore': 1, 'reaction': 1, 'pneumonia': 1, 'tube': 1, 'difficile': 1, 'fluid': 1, 'infection': 1, 'distress': 1, 'sepsis': 1, 'breath': 1, 'vein': 1, 'thrombosis': 1, 'sugar': 1, 'consult': 1, 'status': 1, 'eruption': 1, 'rash': 1, 'room': 1, 'rbc': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eWGNva49frX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}